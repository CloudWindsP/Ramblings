论文项目主页: [https://video-zero-shot.github.io](https://link.zhihu.com/?target=https%3A//video-zero-shot.github.io/)

论文原文 (arXiv): [arXiv:2509.20328v1](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2509.20328)

以上为论文，以下是我会提到的我之前写到的文章

第一性原理：从故事到万物的通用思维模型(一) - 白色书的文章 - 知乎

[第一性原理：从故事到万物的通用思维模型(一)](https://zhuanlan.zhihu.com/p/1953309331203297298)

一些思考(二)写于即将到来的分类学危机前，从家族相似到家族模糊 - 白色书的文章 - 知乎

[一些思考(二)写于即将到来的分类学危机前，从家族相似到家族模糊](https://zhuanlan.zhihu.com/p/1956011751817576543)

以下是个人观点以及我所推测的事情，不一定准确，我提的方案在当前如果用Deepseek，我个人感觉可以做到了，但是我本人没有资源实现，所以只是猜想。

我个人认为DeepMind在视频上的突破是巨大的，他教会了大模型如何生成视频，让视频模型初步确立了在Z轴上的投影——即一种萌芽状态的、然而对于人类而已，如果我们希望大模型按照我们的期望生成，这种只是内部的帧链依旧是不够的，我们需要给大模型加上一个外置的白盒状态机，让人类给定一个向量开始状态和结束状态，并且标明每个向量在第几分钟开始和第几分钟结束，要求大模型生成时也必须比对这些向量保证前后一致，这样才可能从根本上保证能稳定生成几十分钟的视频，我认为是接下来的视频生成方向。

一、帧链是什么，他在原文当中是什么意思。

在解释帧链是什么，为什么这样就能够稳定的生成视频，以及为什么我们依旧需要白盒向量状态机监控前，我们需要说明帧链是什么，以及为什么即使有了帧链，我们依旧需要一个外置的元坐标白盒状态机，用图片这种视觉信号(ControlNet)和Lora，直接约束模型输出，仍然无法精准输出我们想要的。

首先是，什么是帧链，我们按照原文的说法。

Since these changes are applied frame-by-frame in a generated video, this parallels chain-of-thought in LLMs and could therefore be called**chain-of-frames****, or CoF for short**.

**（由于这些变化是在生成的视频中逐帧应用的，这与LLM中的思维链相平行，因此可以被称为帧链或简称CoF。）**

接下来论文反复将CoF与LLM中的思维链进行类比，以强调其作为推理过程而非单纯输出过程的性质。

In the language domain, chain-of-thought enabled models to tackle reasoning problems [78]. Similarly,**chain-of-frames (a.k.a. video generation) might enable video models to solve challenging visual problems that require step-by-step reasoning across time and space.**

**（在语言领域，思维链使得模型能够解决推理问题。类似地，帧链（即视频生成）可能使视频模型能够解决那些需要跨时空进行逐步推理的、具有挑战性的视觉问题。）**

论文通过多个任务来展示CoF在实践中的运作：

- **迷宫求解**：模型必须**逐步地、连贯地**移动一个物体，在每一帧中根据当前状态（位置）决定下一动作（移动方向），最终到达终点。这是一个典型的需要时空规划和执行的CoF过程。
- **机器人导航**：与迷宫类似，但场景更复杂。
- **视觉规则外推**：模型需要从给定的视觉示例中推断出抽象规则，并在新的网格中**一步步应用**该规则来完成图案。
- **物体提取与排列**：模型需要识别所有动物，然后将它们**逐个**移动到正确位置排成一排。

在这些任务中，成功的视频都展示了一个清晰的、逻辑连贯的帧序列，其中每一帧都是基于前一帧状态计算出的下一步。

也就是说所谓的帧链原理仍然是每一帧都是基于前一帧状态计算出的下一步，基于此，我们似乎只要借助ControlNet和Lora就行了，然而我们这里依旧还是存在问题。

我们依然没有告诉想清楚我们要这么告诉大模型我们想要什么。

在智能交互哲学的部分中，我会谈到维特根斯坦的狮子问题，详细阐述为什么**我们仍然需要和智能体的交互协议**，但是还在协众，如若有兴趣的朋友，可以先看看，在一些思考(二)写于即将到来的分类学危机前，从家族相似到家族模糊，我在这里其实已经详细的说明了，我们为什么需要一种新的，和大模型交互的语言。

但是我看见原文依旧提倡用提示词，我认为是不稳定的，因此，在这里我提出XYZ元模型作为一个外置的元坐标白盒状态机。

现有控制方法（如LoRA、ControlNet）是在**视觉特征空间**中进行控制，而XYZ元模型是在**语义概念空间**中进行控制。这是根本区别。

**核心原理：通过XYZ，为生成过程引入一个外部的、基于语义的物理引擎或剧情裁判。**

这个引擎不渲染像素，只处理逻辑和状态。下面我们分步拆解：

**第一步：打标签 —— 建立状态白盒**

在生成开始前，我们不只写一个剧本，而是**为剧本中的关键实体建立一张属性状态表**。

通俗的讲就是每一个关键实体都要有个XYZ的建模。

| 实体     | 初始状态 (T0)                                                | 目标状态 (Tn)                                                |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 英雄     | {Y: 0.1, X: 0.9, Z: 0.95}   (云雾, 客体, 真诚)   (迷茫，但信念源于客观现实，行动真诚) | {Y: 0.9, X: 0.8, Z: 0.7}   (钟表, 客体, 略带解构)   (坚定，信念稳固，但懂得了灵活变通) |
| 反派     | {Y: 0.8, X: 0.3, Z: 0.2}   (钟表, 网络, 解构)   (秩序井然，信念由谎言构建，行动充满算计) | {Y: 0.2, X: 0.9, Z: 0.8}   (云雾, 客体, 真诚)   (计划崩溃，认清现实，忏悔) |
| 魔法剑   | {Y: 0.5, X: 0.5, Z: 0.5}   (平衡态)                          | {Y: 0.9, X: 0.9, Z: 0.9}   (完全觉醒)                        |
| 场景氛围 | {Y: 0.3, X: 0.4, Z: 0.6}   (压抑，不确定)                    | {Y: 0.8, X: 0.8, Z: 0.8}   (明朗，充满希望)                  |

**这一步的意义**：将模糊的英雄成长这个概念，转化为了**可量化的、在三维空间中一条明确的轨迹**。这实现了 **白盒化，**如果中间出了问题不满意，我们可以改变增加向量进行调整。

**第二步：生成与回顾 —— 双系统决策**

这是最关键的一步。生成每一帧时，不再是模型自由发挥，而是一个**受控的决策过程**。

**传统生成：** 上一帧 + 文本提示词 -> 生成下一帧

**XYZ控制生成：** 上一帧 + 文本提示词 + 【来自状态机的合理性约束】 -> 生成下一帧

这个 **合理性约束** 就是整个状态机的原理核心：**必须回顾这一帧的状态和上一帧的状态变化是否合理**。

具体如何实现？我们假设在生成第 **T+1** 帧时，系统会进行以下校验：

1. **状态预测**：根据剧本，状态机知道在T+1时刻，英雄的Y轴值应该从0.3上升到0.4（他变得更坚定了一点）。
2. **状态校验**：系统（可以是一个VLM分析器）会分析**刚刚生成的T+1帧画面**，评估英雄表现出的坚定程度，并给出一个 **Y_observed** 值（比如0.35）。
3. **合理性判决**：      

- **合理**：如果Y_observed (0.35) 与 Y_target (0.4) 在可接受的误差范围内（比如±0.1），则通过。
- **不合理**：如果      Y_observed 是0.1（英雄反而显得更迷茫），则被判定为  **状态跳变不合理**。

**第三步：纠偏 —— 从诊断到治疗**

当合理性判决失败时，这个系统不会将错就错，而是会**主动纠偏**。

**纠偏机制如下：**

1. **诊断报告**：状态机生成诊断报告：第T+1帧错误：英雄‘坚定度’(Y轴)严重偏离预期。当前值0.1，目标值0.4。
2. **强化提示**：系统将原始提示词英雄走在路上，强化为：      英雄走在路上，表情坚定，目光炯炯有神，步伐沉稳有力*（这个强化提示词，可以由LLM根据状态偏差自动生成）*
3. **重新生成**：将强化后的提示词送回视频生成模型，重新生成这一帧，直到其通过合理性检查。

当然了，以上想法仍然太贵，我有一个工程的方法，或许在现在就能实现

我们不要追求全自动的XYZ向量追踪。而是先做一个**给人类导演用的“AI场记板”**。

通过LLM系统分析剧本，生成一个**分镜列表**，每个分镜用文字描述关键的XYZ状态。

人类导演根据这个列表，**手动**选择对应的LoRA和ControlNet参数，进行生成。

这已经能极大提升复杂项目的制作效率和一致性了，至于说内置白盒监控，需要视频大模型开放相关的API才能稳定调整，我想以后会开放相关的。

不过我个人评估了下，语义XYZ在生成视频太模糊了，换成图像XYZ会比较好，大连理工大学等提到的 CineMaster 这种·外部三维控制系统完全可以和图像XYZ结合起来，第一个调节外部三维控制系统,第二个写套规则，要求外部三维控制系统按照给给定的语义规则调整图像位置光影然后拍照记录图像XYZ 两者应该能相对较低成本实现我里面说的工程方法，当然了也是相对情况。

这是XYZ元模型的初步使用，有兴趣的朋友可以去看看第一性原理：从故事到万物的通用思维模型(一)，矩阵的关系之类更复杂的东西，里面有论述。